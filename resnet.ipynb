{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1oY3Qc0xhM1xnFxB9KGAEu93suUnwzyam",
      "authorship_tag": "ABX9TyOlENcpWtWntqexfZzMloha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siasmaan/Brain_tumor_DL/blob/colab/resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKce1SM6voVH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install super-gradients &> /dev/null\n",
        "!pip install torchinfo &> /dev/null\n",
        "!pip install imutils &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djj_PbVyQ5uj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import random\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import pprint\n",
        "import torch\n",
        "import pathlib\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from torchinfo import summary\n",
        "from pathlib import Path, PurePath\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from typing import List, Tuple\n",
        "import super_gradients\n",
        "from super_gradients.training import models\n",
        "from super_gradients.training import dataloaders\n",
        "from super_gradients.training import Trainer\n",
        "from super_gradients.training import training_hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c12TWrkPj7l"
      },
      "outputs": [],
      "source": [
        "class config:\n",
        "    EXPERIMENT_NAME = 'resnet_in_action'\n",
        "    MODEL_NAME = 'resnet50'\n",
        "    CHECKPOINT_DIR = 'checkpoints'\n",
        "\n",
        "    # specify the paths to training and validation set\n",
        "    TRAIN_DIR = 'datasets/Datasets/Training'\n",
        "    VAL_DIR = 'datasets/Datasets/Testing'\n",
        "\n",
        "    # set the input height and width\n",
        "    INPUT_HEIGHT = 224\n",
        "    INPUT_WIDTH = 224\n",
        "\n",
        "    # set the input height and width\n",
        "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "    NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    FLIP_PROB = 0.25\n",
        "    ROTATION_DEG = 15\n",
        "    JITTER_PARAM = 0.25\n",
        "    BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zdJEWC6aBXP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "b530bc8c-2507-4a52-e8b3-49f5a3afdb6e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-bd2239108826>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPERIMENT_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_root_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/super_gradients/training/sg_trainer/sg_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, experiment_name, device, multi_gpu, ckpt_root_dir)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# This should later me removed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmulti_gpu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             raise KeyError(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;34m\"Trainer does not accept anymore 'device' and 'multi_gpu' as argument. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;34m\"Both should instead be passed to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Trainer does not accept anymore 'device' and 'multi_gpu' as argument. Both should instead be passed to super_gradients.setup_device(device=..., multi_gpu=..., num_gpus=...)\""
          ]
        }
      ],
      "source": [
        "trainer = Trainer(experiment_name=config.EXPERIMENT_NAME, ckpt_root_dir=config.CHECKPOINT_DIR, device=config.DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdiK2ni8iIPB"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    val_dir: str,\n",
        "    train_transform: transforms.Compose,\n",
        "    val_transform:  transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=config.NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and validation DataLoaders.\n",
        "  Args:\n",
        "    train_dir: Path to training data.\n",
        "    val_dir: Path to validation data.\n",
        "    transform: Transformation pipeline.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, val_dataloader, class_names).\n",
        "  \"\"\"\n",
        "  # Use ImageFolder to create dataset\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "  val_data = datasets.ImageFolder(val_dir, transform=val_transform)\n",
        "\n",
        "  print(f\"[INFO] training dataset contains {len(train_data)} samples...\")\n",
        "  print(f\"[INFO] validation dataset contains {len(val_data)} samples...\")\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "  print(f\"[INFO] dataset contains {len(class_names)} labels...\")\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  print(\"[INFO] creating training and validation set dataloaders...\")\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  val_dataloader = DataLoader(\n",
        "      val_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, val_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WcfjFryMBTo"
      },
      "outputs": [],
      "source": [
        "# initialize our data augmentation functions\n",
        "resize = transforms.Resize(size=(config.INPUT_HEIGHT,config.INPUT_WIDTH))\n",
        "\n",
        "horizontal_flip = transforms.RandomHorizontalFlip(p=config.FLIP_PROB)\n",
        "\n",
        "\n",
        "random_crop = transforms.RandomCrop(size=(config.INPUT_HEIGHT,config.INPUT_WIDTH))\n",
        "\n",
        "norm = transforms.Normalize(mean=config.IMAGENET_MEAN, std=config.IMAGENET_STD)\n",
        "\n",
        "make_tensor = transforms.ToTensor()\n",
        "\n",
        "# initialize our training and validation set data augmentation pipeline\n",
        "train_transforms = transforms.Compose([resize, random_crop, horizontal_flip,  make_tensor, norm])\n",
        "val_transforms = transforms.Compose([resize, make_tensor, norm])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hynr8da3gYO8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "114d3cc2-35ff-4f62-81a8-81555f7caad5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-9ec4a3541730>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_dataloader, valid_dataloader, class_names = create_dataloaders(train_dir=config.TRAIN_DIR,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                      \u001b[0mval_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVAL_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                      \u001b[0mtrain_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                      \u001b[0mval_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_transforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                      batch_size=config.BATCH_SIZE)\n",
            "\u001b[0;32m<ipython-input-24-75beec2821ae>\u001b[0m in \u001b[0;36mcreate_dataloaders\u001b[0;34m(train_dir, val_dir, train_transform, val_transform, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \"\"\"\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# Use ImageFolder to create dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/Datasets/Training'"
          ]
        }
      ],
      "source": [
        "train_dataloader, valid_dataloader, class_names = create_dataloaders(train_dir=config.TRAIN_DIR,\n",
        "                                                                     val_dir=config.VAL_DIR,\n",
        "                                                                     train_transform=train_transforms,\n",
        "                                                                     val_transform=val_transforms,\n",
        "                                                                     batch_size=config.BATCH_SIZE)\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_img(img):\n",
        "    plt.figure(figsize=(20,16))\n",
        "    img = img * 0.5 + 0.5\n",
        "    npimg = np.clip(img.numpy(), 0., 1.)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "data = iter(train_dataloader)\n",
        "images, labels = data.next()\n",
        "show_img(torchvision.utils.make_grid(images))"
      ],
      "metadata": {
        "id": "JqTqbctB45sj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "ced1cee9-3d24-4ce1-9bf9-9fbdac53037e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-027289192b29>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mshow_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50_imagenet_model = models.get(model_name=config.MODEL_NAME, num_classes=NUM_CLASSES, pretrained_weights=\"imagenet\")"
      ],
      "metadata": {
        "id": "FZ8S4qaekIJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=resnet50_imagenet_model,\n",
        "        input_size=(32, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "tvh7AaLtkMxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of layers that you want to freeze\n",
        "layers_to_freeze = [resnet50_imagenet_model.conv1, resnet50_imagenet_model.bn1, resnet50_imagenet_model.layer1, resnet50_imagenet_model.layer2]\n",
        "\n",
        "# Loop through the list of layers and set the requires_grad attribute to False for each layer\n",
        "for layer in layers_to_freeze:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "j7RkzJYjkSFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=resnet50_imagenet_model,\n",
        "        input_size=(32, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "KYKy_EeOkUYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eRe0hBz4G1n"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "training_params =  training_hyperparams.get(\"training_hyperparams/imagenet_resnet50_train_params\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRZhFnEk8XzL"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(\"Training parameters\")\n",
        "pprint.pprint(training_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_params[\"max_epochs\"] = 4\n",
        "training_params[\"ema\"] = True\n",
        "training_params[\"criterion_params\"] = {'smooth_eps': 0.1} # Enable label-smoothing cross-entropy"
      ],
      "metadata": {
        "id": "VvE9oLhLkii6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ojnc1bk9L3s"
      },
      "outputs": [],
      "source": [
        "trainer.train(model=resnet50_imagenet_model,\n",
        "              training_params=training_params,\n",
        "              train_loader=train_dataloader,\n",
        "              valid_loader=valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kijRaMWBItqZ"
      },
      "outputs": [],
      "source": [
        "# Load the best model that we trained\n",
        "best_model = models.get(config.MODEL_NAME,\n",
        "                        num_classes=NUM_CLASSES,\n",
        "                        checkpoint_path=os.path.join(trainer.checkpoints_dir_path,\"ckpt_best.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(model=best_model, test_loader=valid_dataloader, test_metrics_list=['Accuracy','Top5'])"
      ],
      "metadata": {
        "id": "ACrtRZNJe0ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random list of image paths from test set\n",
        "num_images_to_plot = 30\n",
        "test_image_path_list = list(Path(config.VAL_DIR).glob(\"*/*.jpg\")) # get list all image paths from test data\n",
        "test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n",
        "                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n",
        "\n",
        "# set up subplots\n",
        "num_rows = int(np.ceil(num_images_to_plot / 5))\n",
        "fig, ax = plt.subplots(num_rows, 5, figsize=(15, num_rows * 3))\n",
        "ax = ax.flatten()\n",
        "\n",
        "# Make predictions on and plot the images\n",
        "for i, image_path in enumerate(test_image_path_sample):\n",
        "    pred_and_plot_image(model=best_model,\n",
        "                        image_path=image_path,\n",
        "                        class_names=class_names,\n",
        "                        subplot=(num_rows, 5, i+1),  # subplot tuple for `subplot()` function\n",
        "                        image_size=(config.INPUT_HEIGHT, config.INPUT_WIDTH))\n",
        "\n",
        "# adjust spacing between subplots\n",
        "plt.subplots_adjust(wspace=1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L3q_04r_6q-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def pred_and_plot_image(model: torch.nn.Module,\n",
        "                        image_path: str,\n",
        "                        class_names: List[str],\n",
        "                        subplot: Tuple[int, int, int],  # subplot tuple for `subplot()` function\n",
        "                        image_size: Tuple[int, int] = (config.INPUT_HEIGHT, config.INPUT_WIDTH),\n",
        "                        transform: torchvision.transforms = None,\n",
        "                        device: torch.device=config.DEVICE):\n",
        "\n",
        "    if isinstance(image_path, pathlib.PosixPath):\n",
        "        img = Image.open(image_path)\n",
        "    else:\n",
        "        img = Image.open(requests.get(image_path, stream=True).raw)\n",
        "\n",
        "    # create transformation for image (if one doesn't exist)\n",
        "    if transform is None:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=config.IMAGENET_MEAN,\n",
        "                                 std=config.IMAGENET_STD),\n",
        "        ])\n",
        "    transformed_image = transform(img)\n",
        "\n",
        "    # make sure the model is on the target device\n",
        "    model.to(device)\n",
        "\n",
        "    # turn on model evaluation mode and inference mode\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        # add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
        "        transformed_image = transformed_image.unsqueeze(dim=0)\n",
        "\n",
        "        # make a prediction on image with an extra dimension and send it to the target device\n",
        "        target_image_pred = model(transformed_image.to(device))\n",
        "\n",
        "    # convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "    # convert prediction probabilities -> prediction labels\n",
        "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "    # actual label\n",
        "    ground_truth = PurePath(image_path).parent.name\n",
        "\n",
        "    # plot image with predicted label and probability\n",
        "    plt.subplot(*subplot)\n",
        "    plt.imshow(img)\n",
        "    if isinstance(image_path, pathlib.PosixPath):\n",
        "        title = f\"Ground Truth: {ground_truth} | Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n",
        "    else:\n",
        "        title = f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n",
        "    plt.title(\"\\n\".join(textwrap.wrap(title, width=20)))  # wrap text using textwrap.wrap() function\n",
        "    plt.axis(False)"
      ],
      "metadata": {
        "id": "CeZ835VC6itN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}